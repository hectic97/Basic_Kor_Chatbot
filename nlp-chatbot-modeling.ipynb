{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install konlpy","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting konlpy\n  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n\u001b[K     |████████████████████████████████| 19.4 MB 490 kB/s eta 0:00:01    |██                              | 1.2 MB 4.5 MB/s eta 0:00:05     |██████▋                         | 4.0 MB 4.5 MB/s eta 0:00:04     |████████████                    | 7.3 MB 4.5 MB/s eta 0:00:03     |████████████████                | 9.7 MB 4.5 MB/s eta 0:00:03     |█████████████████               | 10.3 MB 4.5 MB/s eta 0:00:03\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.7/site-packages (from konlpy) (1.18.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from konlpy) (0.4.3)\nRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (4.5.0)\nCollecting beautifulsoup4==4.6.0\n  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n\u001b[K     |████████████████████████████████| 86 kB 3.7 MB/s  eta 0:00:01\n\u001b[?25hCollecting JPype1>=0.7.0\n  Downloading JPype1-1.2.0-cp37-cp37m-manylinux2010_x86_64.whl (453 kB)\n\u001b[K     |████████████████████████████████| 453 kB 13.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.1)\nCollecting tweepy>=3.7.0\n  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.14.0)\nRequirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.2.0)\nRequirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.23.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\nInstalling collected packages: tweepy, JPype1, beautifulsoup4, konlpy\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.9.0\n    Uninstalling beautifulsoup4-4.9.0:\n      Successfully uninstalled beautifulsoup4-4.9.0\nSuccessfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 konlpy-0.5.2 tweepy-3.10.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom konlpy.tag import Okt\n\nFILTERS=\"([~,.!?\\\"':;)(])\"\nPAD = \"<PAD>\"\nSTD = \"<SOS>\"\nEND = \"<END>\"\nUNK = \"<UNK>\"\n\nPAD_INDEX = 0\nSTD_INDEX = 1\nEND_INDEX = 2\nUNK_INDEX = 3\n\nMARKER = [PAD, STD, END, UNK]\nCHANGE_FILTER = re.compile(FILTERS)\n\nMAX_SEQUENCE = 25\n\ndef load_data(path):\n    data_df = pd.read_csv(path, header=0)\n    question, answer = list(data_df['Q']), list(data_df['A'])\n\n    return question, answer\n\ndef data_tokenizer(data):\n    words = []\n    for sentence in data:\n        sentence = re.sub(CHANGE_FILTER,\"\",sentence)\n        for word in sentence.split():\n            words.append(word)\n\n    return [word for word in words if word]\n\ndef prepro_like_morphlized(data):\n    morph_analyzer = Okt()\n    result_data = list()\n    for seq in data:\n        morphlized_seq = morph_analyzer.morphs(seq.replace(' ',''))\n        result_data.append(morphlized_seq)\n\n    return result_data\n\ndef load_vocabulary(path, vocab_path):\n    vocabulary_list = list()\n    if not os.path.exists(vocab_path):\n        if (os.path.exists(path)):\n            data_df = pd.read_csv(path, encoding='utf-8')\n            question, answer = list(data_df['Q']), list(data_df['A'])\n            data = []\n            data.extend(question)\n            data.extend(answer)\n\n            words = data_tokenizer(data)\n            words = list(set(words))\n            words[:0] = MARKER # HEAD APPEND\n\n        with open(vocab_path, 'w',encoding='utf-8') as vocabulary_file:\n            for word in words:\n                vocabulary_file.write(word+'\\n')\n\n    with open(vocab_path,'r',encoding='utf-8') as vocabulary_file:\n        for line in vocabulary_file:\n            vocabulary_list.append(line.strip())\n        \n    word2idx, idx2word = make_vocabulary(vocabulary_list)\n\n    return word2idx, idx2word, len(word2idx)\n\ndef make_vocabulary(vocabulary_list):\n    word2idx = {w:i for i,w in enumerate(vocabulary_list)}\n    idx2word = {i:w for i,w in enumerate(vocabulary_list)}\n\n    return word2idx, idx2word\n\n# word2idx, idx2word, vocab_size = load_vocabulary(PATH, VOCAB_PATH)\n\ndef enc_processing(value, dictionary):\n    sequences_input_index = []\n    sequences_length = []\n\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n        sequence_index = []\n\n        for word in sequence.split():\n            if dictionary.get(word) is not None:\n                sequence_index.extend([dictionary[word]])\n            else:\n                sequence_index.extend([dictionary[UNK]])\n\n        if len(sequence_index) > MAX_SEQUENCE:\n            sequence_index = sequence_index[:MAX_SEQUENCE]\n        \n        sequences_length.append(len(sequences_length))\n\n        # PADDING POST \n        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n        sequences_input_index.append(sequence_index)\n\n    return np.asarray(sequences_input_index), sequences_length\n\ndef dec_output_processing(value, dictionary):\n    sequences_output_index = []\n    sequences_length = []\n\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n\n        if len(sequence_index) > MAX_SEQUENCE:\n            sequence_index = sequence_index[:MAX_SEQUENCE]\n        sequences_length.append(len(sequence_index))\n        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n        sequences_output_index.append(sequence_index)\n    \n    return np.asarray(sequences_output_index), sequences_length\n\ndef dec_target_processing(value, dictionary):\n    sequences_target_index = []\n    for sequence in value:\n        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n        sequence_index = [dictionary[word] for word in sequence.split()]\n        \n        if len(sequence_index) >= MAX_SEQUENCE:\n            sequence_index = sequence_index[:MAX_SEQUENCE-1] + [dictionary[END]]\n        else:\n            sequence_index += [dictionary[END]]\n\n        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n        sequences_target_index.append(sequence_index)\n\n    return np.asarray(sequences_target_index)\n","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import argparse\nimport os\nimport glob\nimport sys\nfrom tqdm import tqdm\n# from preprocess import *\n\n\ndef preprocess_data(PATH, VOCAB_PATH):\n    inputs, outputs = load_data(PATH)\n    char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH)\n    index_inputs, input_seq_len = enc_processing(inputs, char2idx)\n    index_outputs, output_seq_len = dec_output_processing(outputs, char2idx)\n    index_targets = dec_target_processing(outputs, char2idx)\n\n    data_configs = {}\n    data_configs['char2idx'] = char2idx\n    data_configs['idx2char'] = idx2char\n    data_configs['vocab_size'] = vocab_size\n    data_configs['pad_symbol'] = PAD\n    data_configs['std_symbol'] = STD\n    data_configs['end_symbol'] = END\n    data_configs['unk_symbol'] = UNK\n\n    DATA_IN_PATH = './'\n    TRAIN_INPUTS = 'train_inputs.npy'\n    TRAIN_OUTPUTS = 'train_outputs.npy'\n    TRAIN_TARGETS = 'traiN_targets.npy'\n    DATA_CONFIGS = 'data_configs.json'\n\n    np.save(open(DATA_IN_PATH+TRAIN_INPUTS,'wb'),index_inputs)\n    np.save(open(DATA_IN_PATH+TRAIN_OUTPUTS,'wb'),index_outputs)\n    np.save(open(DATA_IN_PATH+TRAIN_TARGETS,'wb'),index_targets)\n\n    json.dump(data_configs, open(DATA_IN_PATH+DATA_CONFIGS,'w'))\n    print(len(input_seq_len),len(output_seq_len))\n    \n\npreprocess_data('../input/kot-chat/ChatbotData .csv','./vocabulary.txt')   ","execution_count":11,"outputs":[{"output_type":"stream","text":"11823 11823\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport matplotlib.pyplot as plt\n\n#from preprocessing import *\nDATA_IN_PATH = './'\nTRAIN_INPUTS = 'train_inputs.npy'\nTRAIN_OUTPUTS = 'train_outputs.npy'\nTRAIN_TARGETS = 'traiN_targets.npy'\nDATA_CONFIGS = 'data_configs.json'\n\ndef plot_graphs(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string],'')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legned([string,'val_'+string])\n    plt.show()\n    \nSEED_NUM = 103\ntf.random.set_seed(SEED_NUM)\n\nindex_inputs = np.load(open(DATA_IN_PATH+TRAIN_INPUTS,'rb'))\nindex_outputs = np.load(open(DATA_IN_PATH+TRAIN_OUTPUTS,'rb'))\nindex_targets = np.load(open(DATA_IN_PATH+TRAIN_TARGETS,'rb'))\nprepro_configs = json.load(open(DATA_IN_PATH+DATA_CONFIGS,'r'))\n\nprint(len(index_inputs),len(index_outputs),len(index_targets))\n","execution_count":13,"outputs":[{"output_type":"stream","text":"11823 11823 11823\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'seq2seq_kor'\nBATCH_SIZE = 2\nMAX_SEQUENCE = 25\nEPOCH = 30\nUNITS = 1024\nEMBEDDING_DIM = 256\nVALIDATION_SPLIT = .1\n\nchar2idx = prepro_configs['char2idx']\nidx2char = prepro_configs['idx2char']\nstd_index = prepro_configs['std_symbol']\nend_index = prepro_configs['end_symbol']\nvocab_size = prepro_configs['vocab_size']\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initalizer='glorot_uniform')\n    \n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state=hidden)\n        return output, state\n    \n    def initalize_hidden_state(self, inp):\n        return tf.zeros((tf.shape(inp)[0], self.enc_units))\n    ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V  = tf.keras.layers.Dense(1)\n        \n    def call(self, query, values):\n        hidden_with_time_axis = tf.expand_dims(query,1)\n        # Consider different Score metrics & Visualizing Attetion score when input come in \n        score = self.V(tf.nn.tanh(self.W1(values)+self.W2(hidden_with_tim_axis)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector,axis=1)\n        return context_vector, attention_weights\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}